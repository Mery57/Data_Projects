{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mery57/Data_Projects/blob/main/PySpark_RDD_groupByKey.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kddBt0lTuY9",
        "outputId": "ca2cda5f-1cc4-4e8e-cf4d-5d2a6ca2f5da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=b1daaa3448a32b271eebb4efe6b35a3325a89fc29da0da6366fd55e1faf374d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import sys\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#-----------------------------------------------------\n",
        "# Apply a groupByKey() transformation to an\n",
        "# RDD[key, value] to find average per key.\n",
        "# Input: NONE\n",
        "#------------------------------------------------------\n",
        "# Input Parameters:\n",
        "#    NONE\n",
        "#-------------------------------------------------------\n",
        "# @author Mahmoud Parsian\n",
        "#-------------------------------------------------------\n",
        "\n",
        "#=========================================\n",
        "def create_pair(t3):\n",
        "    # t3 = (name, city, number)\n",
        "    name = t3[0]\n",
        "    #city = t3[1]\n",
        "    number = int(t3[2])\n",
        "    # return (k, v) pair\n",
        "    return (name, number)\n",
        "#end-def\n",
        "#==========================================\n",
        "def main():\n",
        "\n",
        "    # create an instance of SparkSession\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "    #\n",
        "\n",
        "    #========================================\n",
        "    # groupByKey() transformation\n",
        "    #\n",
        "    # source_rdd.groupByKey() --> target_rdd\n",
        "    #\n",
        "    # Group the values for each key in the RDD\n",
        "    # into a single sequence. Hash-partitions the\n",
        "    # resulting RDD with the existing partitioner/\n",
        "    # parallelism level.\n",
        "    #\n",
        "    # Note: If you are grouping in order to perform\n",
        "    # an aggregation (such as a sum or average) over\n",
        "    # each key, using reduceByKey() or combineByKey()\n",
        "    # will provide much better performance.\n",
        "    #========================================\n",
        "\n",
        "    # Create a list of tuples.\n",
        "    # Each tuple contains name, city, and age.\n",
        "    # Create a RDD from the list above.\n",
        "    list_of_tuples= [('alex','Sunnyvale', 25), \\\n",
        "                     ('alex','Sunnyvale', 33), \\\n",
        "                     ('alex','Sunnyvale', 45), \\\n",
        "                     ('alex','Sunnyvale', 63), \\\n",
        "                     ('mary', 'Ames', 22), \\\n",
        "                     ('mary', 'Cupertino', 66), \\\n",
        "                     ('mary', 'Ames', 20), \\\n",
        "                     ('bob', 'Ames', 26)]\n",
        "    print(\"list_of_tuples = \", list_of_tuples)\n",
        "    rdd = spark.sparkContext.parallelize(list_of_tuples)\n",
        "    print(\"rdd = \", rdd)\n",
        "    print(\"rdd.count() = \", rdd.count())\n",
        "    print(\"rdd.collect() = \", rdd.collect())\n",
        "\n",
        "    #------------------------------------\n",
        "    # apply a map() transformation to rdd\n",
        "    # create a (key, value) pair\n",
        "    #  where\n",
        "    #       key is the name (first element of tuple)\n",
        "    #       value is a number\n",
        "    #------------------------------------\n",
        "    rdd2 = rdd.map(lambda t : create_pair(t))\n",
        "    print(\"rdd2 = \", rdd2)\n",
        "    print(\"rdd2.count() = \", rdd2.count())\n",
        "    print(\"rdd2.collect() = \", rdd2.collect())\n",
        "\n",
        "\n",
        "    #------------------------------------\n",
        "    # apply a groupByKey() transformation to rdd2\n",
        "    # to create a (key, value) pairs (as rdd3)\n",
        "    #  where\n",
        "    #       key is the name\n",
        "    #       value is the Iterable<number>\n",
        "    #------------------------------------\n",
        "    rdd3 = rdd2.groupByKey()\n",
        "    print(\"rdd3 = \", rdd3)\n",
        "    print(\"rdd3.count() = \", rdd3.count())\n",
        "    print(\"rdd3.collect() = \", rdd3.collect())\n",
        "    print(\"rdd3.mapValues().collect() = \", rdd3.mapValues(lambda values: list(values)).collect())\n",
        "\n",
        "    # find average per key\n",
        "    averages = rdd3.mapValues(lambda numbers: float(sum(numbers)) / float(len(numbers)))\n",
        "    print(\"averages = \", averages)\n",
        "    print(\"averages.count() = \", averages.count())\n",
        "    print(\"averages.collect() = \", averages.collect())\n",
        "\n",
        "    # done!\n",
        "    spark.stop()\n",
        "#end-def\n",
        "#=========================================\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaOOSNJ0Tu7O",
        "outputId": "8dbf3e68-c4d1-4551-aa14-02f7044fd64a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list_of_tuples =  [('alex', 'Sunnyvale', 25), ('alex', 'Sunnyvale', 33), ('alex', 'Sunnyvale', 45), ('alex', 'Sunnyvale', 63), ('mary', 'Ames', 22), ('mary', 'Cupertino', 66), ('mary', 'Ames', 20), ('bob', 'Ames', 26)]\n",
            "rdd =  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289\n",
            "rdd.count() =  8\n",
            "rdd.collect() =  [('alex', 'Sunnyvale', 25), ('alex', 'Sunnyvale', 33), ('alex', 'Sunnyvale', 45), ('alex', 'Sunnyvale', 63), ('mary', 'Ames', 22), ('mary', 'Cupertino', 66), ('mary', 'Ames', 20), ('bob', 'Ames', 26)]\n",
            "rdd2 =  PythonRDD[2] at RDD at PythonRDD.scala:53\n",
            "rdd2.count() =  8\n",
            "rdd2.collect() =  [('alex', 25), ('alex', 33), ('alex', 45), ('alex', 63), ('mary', 22), ('mary', 66), ('mary', 20), ('bob', 26)]\n",
            "rdd3 =  PythonRDD[8] at RDD at PythonRDD.scala:53\n",
            "rdd3.count() =  3\n",
            "rdd3.collect() =  [('alex', <pyspark.resultiterable.ResultIterable object at 0x7c60828d0fd0>), ('mary', <pyspark.resultiterable.ResultIterable object at 0x7c60828d0f40>), ('bob', <pyspark.resultiterable.ResultIterable object at 0x7c60828d0e80>)]\n",
            "rdd3.mapValues().collect() =  [('alex', [25, 33, 45, 63]), ('mary', [22, 66, 20]), ('bob', [26])]\n",
            "averages =  PythonRDD[11] at RDD at PythonRDD.scala:53\n",
            "averages.count() =  3\n",
            "averages.collect() =  [('alex', 41.5), ('mary', 36.0), ('bob', 26.0)]\n"
          ]
        }
      ]
    }
  ]
}